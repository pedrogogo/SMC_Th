{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np \n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "import csv\n",
    "\n",
    "#un-comment line below to print whole numpy arrays (note that computing cost will be much higher)\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "#this opens_loads the json files and assign them to their corresponding variables\n",
    "\n",
    "dict_bright = json.load(open(\"bright_analysis.json\", 'rb'))\n",
    "dict_metal = json.load(open(\"metal_analysis.json\",'rb'))\n",
    "dict_hard = json.load(open(\"hard_analysis.json\",'rb'))\n",
    "dict_reverb = json.load(open(\"reverb_analysis.json\",'rb'))\n",
    "dict_rough = json.load(open(\"rough_analysis.json\",'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing CSV files \n",
    "\n",
    "with open('bright.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    bright_list_csv = list(reader)\n",
    "    #removing brackets from list\n",
    "    bright_list = [l[0] for l in bright_list_csv]\n",
    "    \n",
    "with open('warm.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    warm_list_csv = list(reader)\n",
    "    warm_list = [l[0] for l in warm_list_csv]\n",
    "    \n",
    "with open('rough.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    rough_list_csv = map(tuple, reader)\n",
    "    rough_list = [l[0] for l in rough_list_csv]\n",
    "    \n",
    "with open('reverb.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    reverb_list_csv = map(tuple, reader)\n",
    "    reverb_list = [l[0] for l in reverb_list_csv]\n",
    "    \n",
    "with open('clear.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    clear_list_csv = list(reader)\n",
    "    clear_list = [l[0] for l in clear_list_csv]\n",
    "    \n",
    "with open('hollow.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    hollow_list_csv = list(reader)\n",
    "    hollow_list = [l[0] for l in hollow_list_csv]\n",
    "    \n",
    "with open('deep.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    deep_list_csv = list(reader)\n",
    "    deep_list = [l[0] for l in deep_list_csv]\n",
    "    \n",
    "with open('punchy.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    punchy_list_csv = list(reader)\n",
    "    punchy_list = [l[0] for l in punchy_list_csv]\n",
    "    \n",
    "with open('metallic.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    metallic_list_csv = list(reader)\n",
    "    metallic_list = [l[0] for l in metallic_list_csv]\n",
    "    \n",
    "with open('sharp.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    sharp_list_csv = list(reader)\n",
    "    sharp_list = [l[0] for l in sharp_list_csv]\n",
    "    \n",
    "with open('hard.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    hard_list_csv = list(reader) \n",
    "    hard_list = [l[0] for l in hard_list_csv]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hard_list #print if want to see content.change accordingly for other lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hard_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#El n√∫mero de sonidos en tu dataset:\n",
    "nb_sounds = len(set(bright_list + hard_list +warm_list + rough_list + reverb_list + clear_list + hollow_list + deep_list + punchy_list + metallic_list + sharp_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sounds_list = (set(bright_list + hard_list +warm_list + rough_list + reverb_list + clear_list + hollow_list + deep_list + punchy_list + metallic_list + sharp_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"sounds_list.txt\", \"w\") as output:\n",
    "    output.write(str(sounds_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Showing VALUES == 'nan' to be removed later on. \n",
    "for keys, values in dict_bright.iteritems():\n",
    "    print(\"BRIGHT-FreesoundID: %s VALUE: %s\" % (keys, values))\n",
    "    if dict_bright[keys] == 'nan':\n",
    "        dict_bright.iteritems\n",
    "    \n",
    "for keys, values in dict_metal.iteritems():\n",
    "    print(\"METAL-FreesoundID: %s VALUE: %s\" % (keys, values))\n",
    "\n",
    "for keys, values in dict_hard.iteritems():\n",
    "    print(\"HARD VALUES-FreesoundID: %s VALUE: %s\" % (keys, values))\n",
    "    \n",
    "    \n",
    "for keys, values in dict_reverb.iteritems():\n",
    "    print(\"REVERB VALUES-FreesoundID:%s VALUE: %s\" % (keys, values))\n",
    "    \n",
    "for keys, values in dict_rough.iteritems():\n",
    "    print(\"ROUGH VALUES-FreesoundID: %s VALUE: %s\" % (keys, values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bright length: 2745\n",
      "metal length: 2645\n",
      "hard length: 2735\n",
      "reverb length: 2422\n",
      "rough length: 2736\n"
     ]
    }
   ],
   "source": [
    "print \"bright length:\",len(dict_bright)\n",
    "print \"metal length:\",len(dict_metal)\n",
    "print \"hard length:\",len(dict_hard)\n",
    "print \"reverb length:\",len(dict_reverb)\n",
    "print \"rough length:\",len(dict_rough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cleaning_removing \"nan\" values from the dicts\n",
    "clean_dict_bright = filter(lambda k: not np.isnan(dict_bright[k]), dict_bright)\n",
    "clean_dict_metal = filter(lambda k: not np.isnan(dict_metal[k]), dict_metal)\n",
    "clean_dict_hard = filter(lambda k: not np.isnan(dict_hard[k]), dict_hard)\n",
    "clean_dict_reverb = filter(lambda k: not np.isnan(dict_reverb[k]), dict_reverb)\n",
    "clean_dict_rough = filter(lambda k: not np.isnan(dict_rough[k]), dict_rough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bright length: 2745\n",
      "metal length: 2562\n",
      "hard length: 2721\n",
      "reverb length: 2422\n",
      "rough length: 2736\n"
     ]
    }
   ],
   "source": [
    "print \"bright length:\",len(clean_dict_bright)\n",
    "print \"metal length:\",len(clean_dict_metal)\n",
    "print \"hard length:\",len(clean_dict_hard)\n",
    "print \"reverb length:\",len(clean_dict_reverb)\n",
    "print \"rough length:\",len(clean_dict_rough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clean_dict_bright #checking out one of them to see content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2550"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying intersection to all the lists\n",
    "all_ids_intersection=list(set(clean_dict_bright) & set(clean_dict_metal) & set(clean_dict_hard) & set(clean_dict_rough))\n",
    "all_ids_intersection\n",
    "len(all_ids_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating matrix X\n",
    "X = []\n",
    "\n",
    "for fs_id in all_ids_intersection:\n",
    "    feature_vector = [dict_bright[fs_id], dict_metal[fs_id], dict_hard[fs_id],dict_rough[fs_id]]\n",
    "    X.append(feature_vector)\n",
    "len(feature_vector)    \n",
    "X = np.array(X)    \n",
    "#X  #printing out matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2550"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2550, 4)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550\n"
     ]
    }
   ],
   "source": [
    "#confirming it matches in size as supposed to. ID DOES NOT BECAUSE OF THE NAN ISSUE.NEED TO BE DONE BEFORE\n",
    "print len(all_ids_intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "all_ids = list_bright_analysis + list_metal_analysis + list_bright_analysis + list_rough_analysis\n",
    "len (all_ids)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#removing duplicate IDs. NOT NEED IT!\n",
    "\n",
    "all_ids_no_duplicated = []\n",
    "for i in all_ids:\n",
    "    if i not in all_ids_no_duplicated:\n",
    "        all_ids_no_duplicated.append(i)\n",
    "\n",
    "len(all_ids_no_duplicated)\n",
    "all_ids_no_duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = []\n",
    "NB_SOUNDS = len(X)  #here will get same result if using \"all_ids_intersection\" instead of \"X\"\n",
    "NB_LABELS = len(feature_vector)\n",
    "\n",
    "y = np.zeros((NB_SOUNDS, NB_LABELS), dtype=int)\n",
    "\n",
    "for idx, sound_id in enumerate(all_ids_intersection): # recorro todos los sonidos (lineas)\n",
    "    if sound_id in bright_list: # si el sonido es bright\n",
    "        y[idx][0] = 1 # add a 1 for each line (soundid) \"idx\" and the columns (label) 0....\n",
    "    if sound_id in metallic_list: \n",
    "        y[idx][1] = 1 # add a 1 for each line (soundid) \"idx\" and the columns (label) 1....\n",
    "    if sound_id in hard_list: \n",
    "        y[idx][2] = 1 # add a 1 for each line (soundid) \"idx\" and the columns (label) 2....\n",
    "    if sound_id in rough_list: \n",
    "        y[idx][3] = 1 # add a 1 for each line (soundid) \"idx\" and the columns (label) 3....¬¥\n",
    "\n",
    "        \n",
    "#Y = np.array(y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2550, 4)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y  #printing out y matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((510, 4), (510, 4))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train  #cheking X_train matrix values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checking it there are nan values\n",
    "#print(np.isinf(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(np.isnan(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(np.isinf(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(np.isnan(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRAIN\n",
    "clf = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#will need to check later on for optimization purposes\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "\n",
    "clf = GridSearchCV(svr,parameters)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30549019607843136"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X, y, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#example different for precision_recall\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "predicted = [1,2,3,4,5,1,2,1,1,4,5] \n",
    "y_test = [1,2,3,4,5,1,2,1,1,4,1]\n",
    "\n",
    "precision, recall, fscore, support = score(y_test, predicted)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0053475935828877002, 0.0052100118165216459, 0.0053475935828877002)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The recall is the ratio tp / (tp + fn) \n",
    "#where tp is the number of true positives and fn the number of false negatives.\n",
    "#The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "y_true = y_test\n",
    "y_pred = clf.predict(X_test)\n",
    " \n",
    "x1 = recall_score(y_true, y_pred, average='micro') \n",
    "x2 = recall_score(y_true, y_pred, average='macro') \n",
    "x3 = recall_score(y_true, y_pred, average='weighted')\n",
    "x1,x2,x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.5, 0.51871657754010692)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The precision is the ratio tp / (tp + fp) \n",
    "#where tp is the number of true positives and fp the number of false positives. \n",
    "#The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "#from sklearn.metrics import precision_score\n",
    "\n",
    "macro_prec=precision_score(y_true, y_pred, average='macro') \n",
    "weigh_prec= precision_score(y_true, y_pred, average='weighted')\n",
    "micro_prec=precision_score(y_true, y_pred, average='micro')\n",
    "\n",
    "micro_prec, macro_prec, weigh_prec \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ACCURACY\n",
    "\n",
    "y_true = X_train.shape\n",
    "y_pred = X_test.shape\n",
    "accuracy_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     bright       1.00      0.01      0.02        87\n",
      "      metal       1.00      0.01      0.02       107\n",
      "       hard       0.00      0.00      0.00        85\n",
      "      rough       0.00      0.00      0.00        95\n",
      "\n",
      "avg / total       0.52      0.01      0.01       374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TEST....classification report\n",
    "#NEED TO CHECK PRECISION AND RECALL...RESULTS SEEM INCORRECT! (all 100%) NEED TO FIND OUT WHAT IS GOING ON!\n",
    "\n",
    "#from sklearn.metrics import classification_report\n",
    "#y_test = clf.predict(y_test) takes it from above\n",
    "y_pred = clf.predict(X_test)\n",
    "categories = ['bright', 'metal', 'hard', 'rough']\n",
    "print(classification_report(y_test, y_pred, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.01      0.02        87\n",
      "          1       1.00      0.01      0.02       107\n",
      "          2       0.00      0.00      0.00        85\n",
      "          3       0.00      0.00      0.00        95\n",
      "\n",
      "avg / total       0.52      0.01      0.01       374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PEARSON CORRELATIONS\n",
    "#intersection nb_sounds and surrey analisis\n",
    "intersection_nb_surrey = list(set(nb_sounds)&set(all_ids_intersection))\n",
    "import scipy from scipy.stats import pearsonr\n",
    "x = scipy.array([-0.65499887, 2.34644428, 3.0]) y = scipy.array([-1.46049758, 3.86537321, 21.0])\n",
    "r_row, p_value = pearsonr(x, y)\n",
    "\n",
    "r_row, p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
